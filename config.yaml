## config.yaml

training:
  total_steps: 100000            # Number of training steps for base model
  warmup_steps: 4000             # Learning rate warmup
  label_smoothing: 0.1           # Label smoothing factor
  batch:
    max_source_tokens: 25000     # Approx. source tokens per batch
    max_target_tokens: 25000     # Approx. target tokens per batch

model:
  encoder_layers: 6              # Number of encoder blocks
  decoder_layers: 6              # Number of decoder blocks
  d_model: 512                   # Embedding & model hidden size
  d_ff: 2048                     # Feed-forward inner layer size
  num_heads: 8                   # Multi-head attention heads
  dropout_rate: 0.1              # Dropout probability
  share_embedding_and_softmax: true  # Tie input embeddings and pre-softmax weights

optimizer:
  type: Adam
  betas: [0.9, 0.98]
  eps: 1e-9

learning_rate_scheduler:
  type: Noam
  formula: "lr = d_model^{-0.5} * min(step^{-0.5}, step * warmup_steps^{-1.5})"
  warmup_steps: 4000

data:
  language_pair: en-de
  train_dataset: WMT2014
  vocab:
    type: byte-pair
    size: 37000

inference:
  beam_size: 4
  length_penalty_alpha: 0.6
  max_length_offset: 50

hardware:
  gpus: 8