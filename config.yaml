## config.yaml

model:
  # Base Transformer configuration
  encoder_layers: 6
  decoder_layers: 6
  d_model: 512
  d_ff: 2048
  num_heads: 8
  dropout_rate: 0.1
  positional_encoding: sinusoidal
  share_embeddings: true

training:
  # Token-based batch sizing (approx. per step)
  batch_tokens_src: 25000
  batch_tokens_tgt: 25000

  # Total training steps
  max_steps: 100000

  # Learning rate warmup
  warmup_steps: 4000

  # Label smoothing
  label_smoothing: 0.1

optimizer:
  type: adam
  beta1: 0.9
  beta2: 0.98
  eps: 1e-9

lr_scheduler:
  type: inverse_sqrt
  # lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))

evaluation:
  beam_size: 4
  length_penalty: 0.6
  checkpoint_average: 5
  max_output_offset: 50

data:
  # Shared source‐target vocabulary size for En–De
  spm_vocab_size: 37000