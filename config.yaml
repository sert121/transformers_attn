## config.yaml

# Data paths and preprocessing
data:
  train_src: "data/wmt14_en_de/train.en"
  train_tgt: "data/wmt14_en_de/train.de"
  dev_src:   "data/wmt14_en_de/newstest2013.en"
  dev_tgt:   "data/wmt14_en_de/newstest2013.de"
  test_src:  "data/wmt14_en_de/newstest2014.en"
  test_tgt:  "data/wmt14_en_de/newstest2014.de"
  max_seq_length: 100

# Subword vocabulary settings
tokenizer:
  type: "byte-pair"
  vocab_size: 37000
  merges: 37000
  special_tokens:
    - "<pad>"
    - "<s>"
    - "</s>"
    - "<unk>"

# Model architecture hyperparameters
model:
  name: "transformer"
  d_model: 512
  d_ff: 2048
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  max_position_embeddings: 512

# Training configuration
training:
  max_steps: 100000
  batch_tokens: 25000        # approximate tokens per batch (source & target)
  eval_steps: 10000          # dev evaluation interval
  save_steps: 10000          # checkpoint interval
  seed: 42

# Optimizer settings
optimizer:
  type: "Adam"
  betas: [0.9, 0.98]
  epsilon: 1e-9

# Learning rate scheduler
scheduler:
  type: "warmup_inverse_sqrt"
  warmup_steps: 4000

# Regularization
regularization:
  label_smoothing: 0.1

# Inference / Decoding
inference:
  beam_size: 4
  length_penalty: 0.6
  max_length_offset: 50

# Logging and checkpoints
logging:
  log_dir: "logs/"
  checkpoint_dir: "checkpoints/"